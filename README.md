# OCR + Classification Pipeline

A complete pipeline for extracting text from PDF files using **Qwen3-VL-4B** OCR and classifying it into questions/answers/metadata using **Qwen3-4B-Instruct-2507** few-shot learning.

## Features

- ğŸ” **OCR Extraction**: Qwen3-VL-4B vision-language model for accurate text extraction
- ğŸ·ï¸ **Few-Shot Classification**: Qwen3-4B-Instruct with carefully crafted examples
- ğŸ“¦ **Smart Line Merging**: Automatically groups related lines into logical blocks
- âš¡ **Pattern Matching**: Fast regex-based classification for obvious cases
- ğŸ“Š **Organized Output**: Separate files for questions, answers, and full results

## Project Structure

```
Classification/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ocr/                   # Qwen3-VL-4B OCR module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ ocr_extractor.py
â”‚   â”œâ”€â”€ text_processing/       # Text processing and merging
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ text_processor.py
â”‚   â”œâ”€â”€ classification/        # Qwen3-4B-Instruct classifier
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ qwen_classifier.py
â”‚   â””â”€â”€ utils/                 # Utilities (config, logging)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                   # Input PDF files
â”œâ”€â”€ models/                    # Model storage (optional)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml           # Configuration
â”œâ”€â”€ outputs/                   # Classification results
â”œâ”€â”€ logs/                     # Log files
â”œâ”€â”€ pipeline.py              # Main entry point
â”œâ”€â”€ requirements.txt          
â””â”€â”€ README.md
```

## Installation

1. **Install dependencies**:
```bash
pip install -r requirements.txt
```

2. **Install Poppler** (for PDF processing):
   - **Windows**: Download from https://github.com/oschwartz10612/poppler-windows/releases/ and add to PATH
   - **Linux**: `sudo apt-get install poppler-utils`
   - **Mac**: `brew install poppler`

3. Models are automatically downloaded from HuggingFace on first use:
   - `Qwen/Qwen3-VL-4B-Instruct` (~8GB) - OCR
   - `Qwen/Qwen3-4B-Instruct-2507` (~8GB) - Classification

## Usage

### Full Pipeline (PDF â†’ Classification)

```bash
# Process a PDF file
python pipeline.py data/raw/test_2.pdf

# Specify output directory
python pipeline.py data/raw/assignment1.pdf --output results/
```

### Classification Only (Pre-extracted Text)

```bash
# Classify already extracted text
python pipeline.py --text outputs/test_2_extracted.txt
```

### Python API

```python
from src.classification import QwenClassifier

# Initialize classifier
classifier = QwenClassifier()

# Classify text lines
lines = ["Q1: What is AWS?", "Ans: Amazon Web Services", "Page 1"]
results = classifier.classify_document(lines)

print(f"Questions: {len(results['questions'])}")
print(f"Answers: {len(results['answers'])}")
print(f"Metadata: {len(results['metadata'])}")
```

## Output Files

After running the pipeline, you'll find:

```
outputs/
â”œâ”€â”€ {filename}_extracted.txt    # Raw OCR text
â”œâ”€â”€ {filename}_results.json     # Full classification results
â”œâ”€â”€ {filename}_questions.txt    # Extracted questions
â””â”€â”€ {filename}_answers.txt      # Extracted answers
```

### Results JSON Format

```json
{
  "questions": [
    {"text": "Q1: What is...", "confidence": 0.98, "reasoning": "Pattern matched"}
  ],
  "answers": [...],
  "metadata": [...],
  "statistics": {
    "total_original_lines": 174,
    "total_merged_blocks": 102,
    "questions_count": 10,
    "answers_count": 29,
    "metadata_count": 63
  }
}
```

## Classification Categories

| Category | Examples |
|----------|----------|
| **question** | `Q1:`, `Ques-1:`, interrogative sentences, problem scenarios |
| **answer** | `Ans:`, `Answer:`, technical explanations, solutions |
| **metadata** | Student names, page numbers, section headers, dates |

## Configuration

Edit `config/config.yaml` for custom settings:

```yaml
ocr:
  model_name: 'Qwen/Qwen3-VL-4B-Instruct'
  
classification:
  model_name: 'Qwen/Qwen3-4B-Instruct-2507'
```

## Requirements

- Python 3.10+
- CUDA-capable GPU (8GB+ VRAM recommended)
- ~16GB disk space for models
